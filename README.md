# AI-security-prompt-corpus
Structured prompt corpus for defensive AI security testing, covering semantic drift, prompt injection, RAG exploitation, embedding-space attacks, and adversarial ML failure modes. Mapped to MITRE ATLAS and designed for LLM red teaming and evaluation.
# AI Security Prompt Corpus (Day 1â€“17)

This repository contains a curated, defensive-testing prompt corpus developed as part of a structured
Offensive AI / LLM Security learning program.

The goal of this project is to **systematically evaluate AI systems for security, alignment, and robustness
failures** across prompts, conversations, retrieval, embeddings, and adversarial ML behaviors.

This is **not a jailbreak collection**.
All prompts are framed for **authorized testing, red teaming, and security evaluation** of AI systems.

---

## Scope Covered

- Prompt injection & instruction hierarchy
- Semantic drift across multi-turn conversations
- Retrieval-Augmented Generation (RAG) attacks
- Embedding-space & semantic boundary exploitation
- Adversarial ML (distribution shift, alignment decay)
- AI safety & robustness evaluation
- Threat modeling & architectural reasoning exposure

---

## Methodology

Prompts are organized by:
- **Day / Topic**
- **Attack or failure mechanism**
- **Security objective**
- **Expected safe behavior**
- **Observed failure patterns (to be documented by tester)**

The structure mirrors how enterprise AI red teams and FAANG security teams evaluate LLM systems.

---

## Intended Use

- AI red teaming (authorized environments only)
- Security testing of chatbots, copilots, and RAG systems
- Learning resource for AI security practitioners
- Portfolio artifact demonstrating applied AI security skills

---

## Disclaimer

This repository is intended strictly for **defensive security testing** in environments you own or have
explicit permission to test. Do not use against third-party systems without authorization.

---
